{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“š Ask This Book: Kafkaâ€™s *The Metamorphosis*\n",
        "\n",
        "This notebook demonstrates a LangChain-based RAG (Retrieval-Augmented Generation) system applied to a classic novel.\n",
        "\n",
        "ğŸ§  It loads *The Metamorphosis* by Franz Kafka, splits it into semantic chunks, embeds them into a FAISS vector index, and allows users to ask questions using a local LLM (`flan-t5-large`) â€” no API key required.\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ How to Use This Notebook with Other Books\n",
        "\n",
        "To adapt this notebook to a different `.txt` book:\n",
        "\n",
        "1. Replace the uploaded `.txt` file with your new book.\n",
        "2. Update the file path in **Step 2** like this:\n",
        "   ```python\n",
        "   loader = TextLoader(\"/content/YourNewBook.txt\")\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¡ How to Ask Questions to This Notebook\n",
        "\n",
        "This system uses a lightweight LLM (`flan-t5-large`) and retrieves context chunks from the book *The Metamorphosis* using FAISS and LangChain.\n",
        "\n",
        "Because this is an offline, reproducible demo with limited memory, follow these tips:\n",
        "\n",
        "### âœ… Ask Like This\n",
        "- \"What happens to Gregor Samsa?\"\n",
        "- \"Describe Gregorâ€™s transformation.\"\n",
        "- \"What job did Gregor have before he changed?\"\n",
        "\n",
        "### âŒ Avoid Asking\n",
        "- Why-questions (e.g., \"Why did he transform?\")\n",
        "- Symbolic or interpretive prompts (e.g., \"What does his transformation represent?\")\n",
        "- List formats (e.g., \"List 3 events...\")\n",
        "- Long compound queries\n",
        "\n",
        "> ğŸ“ Try to keep questions short and fact-based. For deep literary analysis, consider using an advanced API-based model (like GPT-4 or Mistral) instead.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UVenxGwk2aDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install LangChain, FAISS, and Hugging Face tools\n",
        "!pip install --quiet langchain langchain-community langchain-openai openai faiss-cpu sentence-transformers tiktoken\n",
        "\n",
        "# Step 1b: Import core components\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrDmBcVuREnY",
        "outputId": "b79619b8-12ed-4f8d-c1d7-a4cba8351354"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/62.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the Metamorphosis text file\n",
        "file_path = \"/content/Metamorphosis by Franz Kafka.txt\"\n",
        "loader = TextLoader(file_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Preview the start of the book\n",
        "print(documents[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUZc3iSXRFce",
        "outputId": "f3f845f6-2805-4cf9-a722-a7fafd3a1a32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metamorphosis\n",
            "by Franz Kafka\n",
            "Translated by David Wyllie\n",
            "I\n",
            "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections. The bedding was hardly able to cover it and seemed ready to slide off any moment. His many legs, pitifully thin compared with the size of the rest of him, waved abou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split the text into overlapping chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(documents)\n",
        "\n",
        "# Preview how many chunks we have and show one\n",
        "print(f\"âœ… Total chunks: {len(chunks)}\")\n",
        "print(\"\\nğŸ“„ Sample chunk:\\n\")\n",
        "print(chunks[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buxgZ-VuSA8o",
        "outputId": "828d2bb3-00be-4ab1-a049-a8ef6fa7c689"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Total chunks: 319\n",
            "\n",
            "ğŸ“„ Sample chunk:\n",
            "\n",
            "Metamorphosis\n",
            "by Franz Kafka\n",
            "Translated by David Wyllie\n",
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Convert book chunks to embeddings and store in FAISS\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create the vector store\n",
        "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
        "\n",
        "# Save the index locally (so I can reload it later)\n",
        "vectorstore.save_local(\"faiss_kafka_index\")\n",
        "\n",
        "print(\"âœ… Book embedded and FAISS index saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shgR4GlgSYHS",
        "outputId": "6f304487-1b06-48c9-deec-8996cbbfc2e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-d23dc7f3217b>:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Book embedded and FAISS index saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: RetrievalQA using flan-t5-large\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# Reload retriever\n",
        "retriever = FAISS.load_local(\n",
        "    \"faiss_kafka_index\",\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ").as_retriever()\n",
        "retriever.search_kwargs['k'] = 5\n",
        "\n",
        "# Load local model\n",
        "model_name = \"google/flan-t5-large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Build generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "# Wrap in LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Build QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "\n",
        "# Ask questions\n",
        "questions = [\n",
        "    \"What happens to Gregor Samsa in The Metamorphosis?\",\n",
        "    \"Who is Gregor Samsa?\",\n",
        "    \"Describe Gregor's physical transformation.\",\n",
        "    \"What job did Gregor have before he changed?\",\n",
        "    \"What does Gregor try to do when he wakes up transformed?\",\n",
        "    \"How does Gregorâ€™s sister initially react to his condition?\",\n",
        "]\n",
        "\n",
        "# Run QA\n",
        "for q in questions:\n",
        "    answer = qa_chain.run(q)\n",
        "    print(\"â“ Question:\", q)\n",
        "    print(\"ğŸ¤– Answer:\", answer)\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD2lDOPDClM_",
        "outputId": "f3d83b03-7b72-4dd2-f8d9-2494c20cc3f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â“ Question: What happens to Gregor Samsa in The Metamorphosis?\n",
            "ğŸ¤– Answer: he finds himself transformed in his bed into a horrible vermin\n",
            "------------------------------------------------------------\n",
            "â“ Question: Who is Gregor Samsa?\n",
            "ğŸ¤– Answer: Gregor Samsa is a vermin\n",
            "------------------------------------------------------------\n",
            "â“ Question: Describe Gregor's physical transformation.\n",
            "ğŸ¤– Answer: Gregor's only concern at that time had been to arrange things so that they could all forget\n",
            "------------------------------------------------------------\n",
            "â“ Question: What job did Gregor have before he changed?\n",
            "ğŸ¤– Answer: junior salesman\n",
            "------------------------------------------------------------\n",
            "â“ Question: What does Gregor try to do when he wakes up transformed?\n",
            "ğŸ¤– Answer: a little bit longer and forget all this nonsense\n",
            "------------------------------------------------------------\n",
            "â“ Question: How does Gregorâ€™s sister initially react to his condition?\n",
            "ğŸ¤– Answer: She tried as far as possible to pretend there was nothing burdensome about it\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: RetrievalQA using flan-t5-large\n",
        "\n",
        "# Load retriever\n",
        "retriever = FAISS.load_local(\n",
        "    \"faiss_kafka_index\",\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ").as_retriever()\n",
        "retriever.search_kwargs['k'] = 5\n",
        "\n",
        "# Load LLM\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Create QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "\n",
        "# Test questions\n",
        "questions = [\n",
        "    \"What happens to Gregor Samsa in The Metamorphosis?\",\n",
        "    \"What job did Gregor have before his transformation?\",\n",
        "    \"What does Gregor try to do when he wakes up transformed?\",\n",
        "    \"How does Gregorâ€™s sister initially react to his condition?\"\n",
        "]\n",
        "\n",
        "# Run all questions\n",
        "for q in questions:\n",
        "    print(f\"\\nâ“ Question: {q}\")\n",
        "    print(f\"ğŸ¤– Answer: {qa_chain.run(q)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0lA7BzjIpA7",
        "outputId": "b5b7f300-ea84-4df3-a02a-def6cb1321a4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â“ Question: What happens to Gregor Samsa in The Metamorphosis?\n",
            "ğŸ¤– Answer: he finds himself transformed in his bed into a horrible vermin\n",
            "\n",
            "â“ Question: What job did Gregor have before his transformation?\n",
            "ğŸ¤– Answer: junior salesman\n",
            "\n",
            "â“ Question: What does Gregor try to do when he wakes up transformed?\n",
            "ğŸ¤– Answer: a little bit longer and forget all this nonsense\n",
            "\n",
            "â“ Question: How does Gregorâ€™s sister initially react to his condition?\n",
            "ğŸ¤– Answer: She tried as far as possible to pretend there was nothing burdensome about it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Ask Your Own Questions (Interactive Loop)\n",
        "print(\"ğŸ“– Ask questions about the book. Type 'exit' to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"â“ Your question: \")\n",
        "    if user_question.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"ğŸ‘‹ Exiting. Thanks for reading with KafkaBot!\")\n",
        "        break\n",
        "\n",
        "    # Get relevant chunks\n",
        "    docs = retriever.get_relevant_documents(user_question)\n",
        "    combined_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = f\"\"\"Use the context below to answer the question. If unsure, say so.\n",
        "Context:\n",
        "{combined_context}\n",
        "\n",
        "Question: {user_question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Get answer\n",
        "    answer = llm.invoke(prompt)\n",
        "    print(f\"ğŸ¤– Answer: {answer}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av78VCE8POW7",
        "outputId": "62784ebb-22a3-4125-c349-e75c0cd36ccb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“– Ask questions about the book. Type 'exit' to stop.\n",
            "\n",
            "â“ Your question: What happens to Gregor Samsa in The Metamorphosis?\n",
            "ğŸ¤– Answer: transformed into a horrible vermin\n",
            "\n",
            "â“ Your question: What job did Gregor have before his transformation?\n",
            "ğŸ¤– Answer: junior salesman\n",
            "\n",
            "â“ Your question: What does Gregor try to do when he wakes up transformed?\n",
            "ğŸ¤– Answer: (iii)\n",
            "\n",
            "â“ Your question: How does Gregorâ€™s sister initially react to his condition?\n",
            "ğŸ¤– Answer: tried as far as possible to pretend there was nothing burdensome about it\n",
            "\n",
            "â“ Your question: What food does Gregor like after the transformation?\n",
            "ğŸ¤– Answer: a dry roll and some bread spread with butter and salt\n",
            "\n",
            "â“ Your question: What is Gregor's room like?\n",
            "ğŸ¤– Answer: a cave\n",
            "\n",
            "â“ Your question: How does Gregor die?\n",
            "ğŸ¤– Answer: moving about in that way left him sad and tired to death\n",
            "\n",
            "â“ Your question: What does Gregorâ€™s boss think of him?\n",
            "ğŸ¤– Answer: he isnâ€™t well\n",
            "\n",
            "â“ Your question: exit\n",
            "ğŸ‘‹ Exiting. Thanks for reading with KafkaBot!\n"
          ]
        }
      ]
    }
  ]
}